{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mobilenet v2 ONNX Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.15.2-cp310-cp310-macosx_11_0_arm64.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m439.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/anlu/Library/CloudStorage/OneDrive-NationalChengChiUniversity/Andes/.conda/lib/python3.10/site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: requests in /Users/anlu/Library/CloudStorage/OneDrive-NationalChengChiUniversity/Andes/.conda/lib/python3.10/site-packages (from torchvision) (2.31.0)\n",
      "Collecting torch==2.0.1 (from torchvision)\n",
      "  Downloading torch-2.0.1-cp310-none-macosx_11_0_arm64.whl (55.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/anlu/Library/CloudStorage/OneDrive-NationalChengChiUniversity/Andes/.conda/lib/python3.10/site-packages (from torchvision) (10.0.0)\n",
      "Collecting filelock (from torch==2.0.1->torchvision)\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/52/90/45223db4e1df30ff14e8aebf9a1bf0222da2e7b49e53692c968f36817812/filelock-3.12.3-py3-none-any.whl.metadata\n",
      "  Using cached filelock-3.12.3-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: typing-extensions in /Users/anlu/Library/CloudStorage/OneDrive-NationalChengChiUniversity/Andes/.conda/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (4.5.0)\n",
      "Requirement already satisfied: sympy in /Users/anlu/Library/CloudStorage/OneDrive-NationalChengChiUniversity/Andes/.conda/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (1.12)\n",
      "Collecting networkx (from torch==2.0.1->torchvision)\n",
      "  Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "Collecting jinja2 (from torch==2.0.1->torchvision)\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/anlu/Library/CloudStorage/OneDrive-NationalChengChiUniversity/Andes/.conda/lib/python3.10/site-packages (from requests->torchvision) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/anlu/Library/CloudStorage/OneDrive-NationalChengChiUniversity/Andes/.conda/lib/python3.10/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/anlu/Library/CloudStorage/OneDrive-NationalChengChiUniversity/Andes/.conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/anlu/Library/CloudStorage/OneDrive-NationalChengChiUniversity/Andes/.conda/lib/python3.10/site-packages (from requests->torchvision) (2023.7.22)\n",
      "Collecting typing-extensions (from torch==2.0.1->torchvision)\n",
      "  Obtaining dependency information for typing-extensions from https://files.pythonhosted.org/packages/ec/6b/63cc3df74987c36fe26157ee12e09e8f9db4de771e0f3404263117e75b95/typing_extensions-4.7.1-py3-none-any.whl.metadata\n",
      "  Using cached typing_extensions-4.7.1-py3-none-any.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/anlu/Library/CloudStorage/OneDrive-NationalChengChiUniversity/Andes/.conda/lib/python3.10/site-packages (from jinja2->torch==2.0.1->torchvision) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/anlu/Library/CloudStorage/OneDrive-NationalChengChiUniversity/Andes/.conda/lib/python3.10/site-packages (from sympy->torch==2.0.1->torchvision) (1.3.0)\n",
      "Using cached filelock-3.12.3-py3-none-any.whl (11 kB)\n",
      "Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Installing collected packages: typing-extensions, networkx, jinja2, filelock, torch, torchvision\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.5.0\n",
      "    Uninstalling typing_extensions-4.5.0:\n",
      "      Successfully uninstalled typing_extensions-4.5.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-macos 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed filelock-3.12.3 jinja2-3.1.2 networkx-3.1 torch-2.0.1 torchvision-0.15.2 typing-extensions-4.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anlu/Library/CloudStorage/OneDrive-NationalChengChiUniversity/Andes/.conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/anlu/Library/CloudStorage/OneDrive-NationalChengChiUniversity/Andes/.conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models, datasets, transforms as T\n",
    "mobilenet_v2 = models.mobilenet_v2(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "拿 pretrained 模型：mobilenet_v2_float.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "image_height = 224\n",
    "image_width = 224\n",
    "x = torch.randn(1, 3, image_height, image_width, requires_grad=True)\n",
    "torch_out = mobilenet_v2(x)\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(mobilenet_v2,              # model being run\n",
    "                  x,                         # model input (or a tuple for multiple inputs)\n",
    "                  \"mobilenet_v2_float.onnx\", # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=12,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output']) # the model's output names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import onnxruntime\n",
    "import torch\n",
    "\n",
    "def preprocess_image(image_path, height, width, channels=3):\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize((width, height), Image.NEAREST)\n",
    "    image_data = np.asarray(image).astype(np.float32)\n",
    "    image_data = image_data.transpose([2, 0, 1]) # transpose to CHW\n",
    "    mean = np.array([0.079, 0.05, 0]) + 0.406\n",
    "    std = np.array([0.005, 0, 0.001]) + 0.224\n",
    "    for channel in range(image_data.shape[0]):\n",
    "        image_data[channel, :, :] = (image_data[channel, :, :] / 255 - mean[channel]) / std[channel]\n",
    "    image_data = np.expand_dims(image_data, 0)\n",
    "    return image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 10472  100 10472    0     0  17396      0 --:--:-- --:--:-- --:--:-- 17511\n"
     ]
    }
   ],
   "source": [
    "# Download ImageNet labels\n",
    "!curl -o imagenet_classes.txt https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n",
    "\n",
    "# Read the categories\n",
    "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
    "    categories = [s.strip() for s in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "開始推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combination lock 0.094247974\n",
      "padlock 0.09198424\n",
      "coffeepot 0.07444616\n",
      "coffee mug 0.059318908\n",
      "espresso maker 0.054574847\n"
     ]
    }
   ],
   "source": [
    "session_fp32 = onnxruntime.InferenceSession(\"mobilenet_v2_float.onnx\")\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def run_sample(session, image_file, categories):\n",
    "    output = session.run([], {'input':preprocess_image(image_file, image_height, image_width)})[0]\n",
    "    output = output.flatten()\n",
    "    output = softmax(output) # this is optional\n",
    "    top5_catid = np.argsort(-output)[:5]\n",
    "    for catid in top5_catid:\n",
    "        print(categories[catid], output[catid])\n",
    "\n",
    "run_sample(session_fp32, 'test_data/beer.JPG', categories)\n",
    "# run_sample(session_fp32, 'test.JPG', categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 量化 Quantization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.quantization import quantize_static, CalibrationDataReader, QuantType\n",
    "import os\n",
    "\n",
    "def preprocess_func(images_folder, height, width, size_limit=0):\n",
    "    image_names = os.listdir(images_folder)\n",
    "    if size_limit > 0 and len(image_names) >= size_limit:\n",
    "        batch_filenames = [image_names[i] for i in range(size_limit)]\n",
    "    else:\n",
    "        batch_filenames = image_names\n",
    "    unconcatenated_batch_data = []\n",
    "\n",
    "    for image_name in batch_filenames:\n",
    "        image_filepath = images_folder + '/' + image_name\n",
    "        image_data = preprocess_image(image_filepath, height, width)\n",
    "        unconcatenated_batch_data.append(image_data)\n",
    "    batch_data = np.concatenate(np.expand_dims(unconcatenated_batch_data, axis=0), axis=0)\n",
    "    return batch_data\n",
    "\n",
    "\n",
    "class MobilenetDataReader(CalibrationDataReader):\n",
    "    def __init__(self, calibration_image_folder):\n",
    "        self.image_folder = calibration_image_folder\n",
    "        self.preprocess_flag = True\n",
    "        self.enum_data_dicts = []\n",
    "        self.datasize = 0\n",
    "\n",
    "    def get_next(self):\n",
    "        if self.preprocess_flag:\n",
    "            self.preprocess_flag = False\n",
    "            nhwc_data_list = preprocess_func(self.image_folder, image_height, image_width, size_limit=0)\n",
    "            self.datasize = len(nhwc_data_list)\n",
    "            self.enum_data_dicts = iter([{'input': nhwc_data} for nhwc_data in nhwc_data_list])\n",
    "        return next(self.enum_data_dicts, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocess:\n",
    "- Symbolic Shape Inference. It works best with transformer models.\n",
    "- ONNX Runtime Model Optimization.\n",
    "- ONNX Shape Inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m onnxruntime.quantization.preprocess --input mobilenet_v2_float.onnx --output mobilenet_v2_float_infer.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "量化模型 mobilenet_v2_float.onnx --> mobilenet_v2_uint8.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_height' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/anlu/Library/CloudStorage/OneDrive-NationalChengChiUniversity/Andes/Pytorch2ONNX/torch_to_onnx.ipynb Cell 15\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anlu/Library/CloudStorage/OneDrive-NationalChengChiUniversity/Andes/Pytorch2ONNX/torch_to_onnx.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m calibration_data_folder \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcalibration_imagenet\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anlu/Library/CloudStorage/OneDrive-NationalChengChiUniversity/Andes/Pytorch2ONNX/torch_to_onnx.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m dr \u001b[39m=\u001b[39m MobilenetDataReader(calibration_data_folder)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/anlu/Library/CloudStorage/OneDrive-NationalChengChiUniversity/Andes/Pytorch2ONNX/torch_to_onnx.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m quantize_static(\u001b[39m'\u001b[39;49m\u001b[39mmobilenet_v2_float_infer.onnx\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anlu/Library/CloudStorage/OneDrive-NationalChengChiUniversity/Andes/Pytorch2ONNX/torch_to_onnx.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                 \u001b[39m'\u001b[39;49m\u001b[39mmobilenet_v2_uint8.onnx\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anlu/Library/CloudStorage/OneDrive-NationalChengChiUniversity/Andes/Pytorch2ONNX/torch_to_onnx.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                 dr)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anlu/Library/CloudStorage/OneDrive-NationalChengChiUniversity/Andes/Pytorch2ONNX/torch_to_onnx.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mONNX full precision model size (MB):\u001b[39m\u001b[39m'\u001b[39m, os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mgetsize(\u001b[39m\"\u001b[39m\u001b[39mmobilenet_v2_float.onnx\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m/\u001b[39m(\u001b[39m1024\u001b[39m\u001b[39m*\u001b[39m\u001b[39m1024\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/anlu/Library/CloudStorage/OneDrive-NationalChengChiUniversity/Andes/Pytorch2ONNX/torch_to_onnx.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mONNX quantized model size (MB):\u001b[39m\u001b[39m'\u001b[39m, os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mgetsize(\u001b[39m\"\u001b[39m\u001b[39mmobilenet_v2_uint8.onnx\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m/\u001b[39m(\u001b[39m1024\u001b[39m\u001b[39m*\u001b[39m\u001b[39m1024\u001b[39m))\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-NationalChengChiUniversity/Andes/.conda/lib/python3.10/site-packages/onnxruntime/quantization/quantize.py:374\u001b[0m, in \u001b[0;36mquantize_static\u001b[0;34m(model_input, model_output, calibration_data_reader, quant_format, op_types_to_quantize, per_channel, reduce_range, activation_type, weight_type, nodes_to_quantize, nodes_to_exclude, optimize_model, use_external_data_format, calibrate_method, extra_options)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[39mwith\u001b[39;00m tempfile\u001b[39m.\u001b[39mTemporaryDirectory(prefix\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mort.quant.\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m quant_tmp_dir:\n\u001b[1;32m    366\u001b[0m     calibrator \u001b[39m=\u001b[39m create_calibrator(\n\u001b[1;32m    367\u001b[0m         model,\n\u001b[1;32m    368\u001b[0m         op_types_to_quantize,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    372\u001b[0m         extra_options\u001b[39m=\u001b[39mcalib_extra_options,\n\u001b[1;32m    373\u001b[0m     )\n\u001b[0;32m--> 374\u001b[0m     calibrator\u001b[39m.\u001b[39;49mcollect_data(calibration_data_reader)\n\u001b[1;32m    375\u001b[0m     tensors_range \u001b[39m=\u001b[39m calibrator\u001b[39m.\u001b[39mcompute_range()\n\u001b[1;32m    376\u001b[0m     \u001b[39mdel\u001b[39;00m calibrator\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-NationalChengChiUniversity/Andes/.conda/lib/python3.10/site-packages/onnxruntime/quantization/calibrate.py:245\u001b[0m, in \u001b[0;36mMinMaxCalibrater.collect_data\u001b[0;34m(self, data_reader)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcollect_data\u001b[39m(\u001b[39mself\u001b[39m, data_reader: CalibrationDataReader):\n\u001b[1;32m    244\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 245\u001b[0m         inputs \u001b[39m=\u001b[39m data_reader\u001b[39m.\u001b[39;49mget_next()\n\u001b[1;32m    246\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m inputs:\n\u001b[1;32m    247\u001b[0m             \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;32m/Users/anlu/Library/CloudStorage/OneDrive-NationalChengChiUniversity/Andes/Pytorch2ONNX/torch_to_onnx.ipynb Cell 15\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/anlu/Library/CloudStorage/OneDrive-NationalChengChiUniversity/Andes/Pytorch2ONNX/torch_to_onnx.ipynb#X20sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess_flag:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/anlu/Library/CloudStorage/OneDrive-NationalChengChiUniversity/Andes/Pytorch2ONNX/torch_to_onnx.ipynb#X20sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess_flag \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/anlu/Library/CloudStorage/OneDrive-NationalChengChiUniversity/Andes/Pytorch2ONNX/torch_to_onnx.ipynb#X20sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     nhwc_data_list \u001b[39m=\u001b[39m preprocess_func(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_folder, image_height, image_width, size_limit\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/anlu/Library/CloudStorage/OneDrive-NationalChengChiUniversity/Andes/Pytorch2ONNX/torch_to_onnx.ipynb#X20sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatasize \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(nhwc_data_list)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/anlu/Library/CloudStorage/OneDrive-NationalChengChiUniversity/Andes/Pytorch2ONNX/torch_to_onnx.ipynb#X20sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menum_data_dicts \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m([{\u001b[39m'\u001b[39m\u001b[39minput\u001b[39m\u001b[39m'\u001b[39m: nhwc_data} \u001b[39mfor\u001b[39;00m nhwc_data \u001b[39min\u001b[39;00m nhwc_data_list])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_height' is not defined"
     ]
    }
   ],
   "source": [
    "# change it to your real calibration data set\n",
    "calibration_data_folder = \"calibration_imagenet\"\n",
    "dr = MobilenetDataReader(calibration_data_folder)\n",
    "\n",
    "quantize_static('mobilenet_v2_float_infer.onnx',\n",
    "                'mobilenet_v2_uint8.onnx',\n",
    "                dr)\n",
    "\n",
    "print('ONNX full precision model size (MB):', os.path.getsize(\"mobilenet_v2_float.onnx\")/(1024*1024))\n",
    "print('ONNX quantized model size (MB):', os.path.getsize(\"mobilenet_v2_uint8.onnx\")/(1024*1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "開始推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beer glass 0.21646532\n",
      "padlock 0.14559749\n",
      "coffee mug 0.07273514\n",
      "beer bottle 0.07273514\n",
      "combination lock 0.059652314\n"
     ]
    }
   ],
   "source": [
    "session_quant = onnxruntime.InferenceSession(\"mobilenet_v2_uint8.onnx\")\n",
    "run_sample(session_quant, 'beer.JPG', categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2onnx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
